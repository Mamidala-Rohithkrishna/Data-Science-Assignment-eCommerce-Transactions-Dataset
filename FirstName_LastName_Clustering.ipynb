# Import necessary libraries
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import davies_bouldin_score
import matplotlib.pyplot as plt
import seaborn as sns

# Load datasets
customers = pd.read_csv('path/to/Customers.csv')
transactions = pd.read_csv('path/to/Transactions.csv')

# Merge datasets to create a combined dataset
customer_transactions = transactions.merge(customers, on='CustomerID')

# Feature engineering
customer_profiles = customer_transactions.groupby('CustomerID').agg({
    'Price': 'sum',
    'Quantity': 'sum',
    'TotalValue': 'sum',
    'Region': lambda x: x.mode()[0]
}).reset_index()

# One-hot encoding for categorical variables
customer_profiles = pd.get_dummies(customer_profiles, columns=['Region'])

# Scale features
scaler = StandardScaler()
scaled_data = scaler.fit_transform(customer_profiles.drop('CustomerID', axis=1))

# Apply clustering (KMeans with 3 clusters as an example)
kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(scaled_data)

# Add cluster labels to the dataset
customer_profiles['Cluster'] = clusters

# Calculate Davies-Bouldin Index
db_index = davies_bouldin_score(scaled_data, clusters)

# Print clustering results
print(f'Number of clusters: 3')
print(f'Davies-Bouldin Index: {db_index:.2f}')

# Visualize clusters
plt.figure(figsize=(10, 6))
sns.scatterplot(x=scaled_data[:, 0], y=scaled_data[:, 1], hue=clusters, palette='viridis')
plt.title('Customer Segmentation Clusters')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend(title='Cluster')
plt.show()

# Save clustering results to CSV
customer_profiles.to_csv('FirstName_LastName_Clustering.csv', index=False)
